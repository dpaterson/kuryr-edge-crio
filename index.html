<!DOCTYPE html>
<html>
  <head>
    <title>
        OpenStack at the Edge - Distributed Compute Nodes (DCN)
    </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="slides.css"></link>
  </head>
  <body>
    <textarea id="source">

layout: true
name: title_layout
class: title_slide

---

layout: true
name: section_layout
class: section_slide

---

layout: true
name: agenda_layout
class: content_slide agenda_slide

---

layout: true
name: thank_you
class: thank_you

---

layout: true
name: content_layout
class: content_slide

---

template: title_layout

# OpenStack at the Edge - Distributed Compute Nodes

David Paterson - Sr. Principal Software Engineer - Dell EMC<br>
<br>
Red Hat Summit, Boston MA, 5.8.2019<br>

---
template: section_layout

Overview

---

# Overview

As of Red Hat OpenStack 13, deploying edge compute node sites is fully
supported.

## What we'll cover:
1. Physical environment
2. Distributed compute nodes
3. Simulated network impairment
4. Network Topology
5. Looking forward: Bare metal at the edge
6. Closing remarks and Q&A

---
# Motivation

* Supporting workloads at the edge is becoming mission critical:

--
 * Lower latency

--
 * Reduction of backhaul

--
 * 5G, MEC, and IoT have many usecases where proximity is clutch

---
# Phased Implementation Strategy

1. Rack & stack hardware and configure physical networking
2. Deploy typical 9 node cluster representing CDC
3. Configure network impairment node
4. Discover and provision remote compute nodes
5. Instantiate instance running simple workload on remote compute
6. Network impairment testing
7. Deploy Ironic at edge site to support bare metal instances
8. Close the loop, make it all work together

We are currently working on 7 - 8

---
template: section_layout

Edge Focused Upstream Openstack Activity

---
# State of OpenStack's support for the edge
There's a lot of activity and also a long way to go:

* OpenStack's support for the edge (near, middle, far, fog ...) is still in a
relatively early stage.

--
* The OpenStack Edge Working Group is doing good work defining use cases and
MVP architectures

<table class="no-padding">
<tr><td>
  .center[.ninety[![Prototype Topology](
  assets/1200px-Large_Scale_Centralized_Control.png)]]
</td><td>
  .center[.ninety[![Prototype Topology](
  assets/800px-Centralized_Control.png)]]
</td></tr>
</table>

---
Our prototype closely resembles the OSF Edge Computing Workgroup's Centralized
Control Plane MVP architecture, sans Cinder

.center[.half[![Centralized Control Plane](
assets/800px-Centralized_Control_Strike_Cinder.png)]]


---
Openstack, Linux Foundation, and other edge related projects
* There are many nascent satellite projects addressing requirements needed for
OpenStack to support the edge

--
 * StarlingX
 * Akraino
 * Airship
 * OPNFV
 * LF EDGE umbrella project
 * ONAP
 * Next week there will be more!

---
# Why it's good?
* The demand for IaaS at the edge is really good for OpenStack
* There is a lot of momentum around edge in the open source community
* These projects are driving OpenStack to be ahead of the curve

---
# Why it's bad?
* Confusion! It's difficult to track what project is doing what...
* Having many projects with overlapping/redundant concerns makes adoption
difficult
* Fragmentation and duplication of efforts, 15 code bases instead of 1

---
template: section_layout

So what can we do right now?

---
# What can we do with OpenStack at the edge right now?

OpenStack already has features we can utilize to:
* Provision edge compute node sites
* Deploy workloads at the edge

---
# Approach to Networking
While there are many permutations of how networking a remote compute site can
be implemented, the following patterns seem to make the most sense:
* Use L3 vs L2 where possible
* Workloads should use a provider network that is only utilized by the remote
compute site
* Don't rely on CDC for workload DHCP, instead:
  * User config_data model for injecting addresses into instances during
  instance boot time (cloud-init)
  * Use site-local DHCP
* Prototyping in a single lab requires some kind of network impairment testing
in order to gauge real-world performace.
* Provisioning remote compute nodes over L3 works, cool!

---
## Prototype Current Network Topology
.center[.seventy[![Prototype Topology](
assets/topology.png)]]

---
# Provisioning compute nodes to edge site

.center[.sixty[![Provisioning Edge Compute Nodes](
assets/edge_provisioning_diagram.png)]]

---
# Deploy a workload to edge compute site

Leverage existing features:
* Nova Host Aggregates and Availability Zones (AZ)
* Roles, flavors and supporting metadata.
* Neutron/OVS composible networks

```
$ openstack aggregate list
+----+----------+-------------------+
| ID | Name     | Availability Zone |
+----+----------+-------------------+
|  2 | betty-ag | betty             |
|  5 | al-ag    | al                |
+----+----------+-------------------+
$ openstack aggregate show al-ag
+-------------------+-------------------------------------------+
| Field             | Value                                     |
+-------------------+-------------------------------------------+
| availability_zone | al                                        |                                  |
| hosts             | [u'r196-dell-edge-compute-0.localdomain'] |
| id                | 5                                         |
| name              | al-ag                                     |
+-------------------+-------------------------------------------+
```

---
## Create an instance on edge node by specifying AZ

```
$ openstack server create --image centos --availability-zone al edge_centos_1
+-------------------------------------+------------------------------------+
| Field                               | Value                              |
+-------------------------------------+------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                             |
| OS-EXT-AZ:availability_zone         | al                                 |
| OS-EXT-SRV-ATTR:host                | edge-compute-0.localdomain         |
| OS-EXT-SRV-ATTR:hypervisor_hostname | edge-compute-0.localdomain         |
| addresses                           | edge-provider=10.30.1.18;          |
|                                     | edge-internet=10.40.1.16           |
| flavor                              | grape (99498278-10af-4f05-937)     |
| hostId                              | 23b36a6630a2c00190ea3915fcf95      |
| id                                  | 66c185b8-3556-4fe0-8d42-6          |
| image                               | centos (bf484078-1f71-4ba3-8e58-5) |
| name                                | edge_centos_1                      |
| progress                            | 0                                  |
| status                              | ACTIVE                             |
+-------------------------------------+------------------------------------+
```
---
# Workloads running in compute nodes at edge - where we are now

<br/><br/>

.center[.full[![Prototype Topology](
assets/dcn_vm_workload_with_ref_arch.png)]]

---
# Workloads running in bare-metal instances - where we are headed
.center[.eighty[![Prototype Topology](
assets/dcn_bm_workload_snippet.png)]]

Ideally we'll be able to run K8S on on bare-metal instances at edge sites.

---
template: section_layout

Limitations

---
# Limitations for remote compute nodes

* Control plane communication latency

--
 * Bad things start happening when round-trip time (RTT) exceeds 100ms

--
* Compute and networking only explored thus far
 * Ephemeral storage only

--
* If the link to the control plane goes down workloads will continue to run but
that's about it until connectivity to control plan is restored

---
template: section_layout

Network Impairment

---
# Simulated Network Impairment
To simulate real world latency between the control plane and remote compute
site some kind of simulated network impairment is needed.

* Ideally all L3 traffic goes through impairment device
* If you are doing any L2 networking (VLAN) testing may have to happen
directly on controller nodes
* Pretty simple to implement
 * Routing
 * Simple bash scripts that control latency

---
# Impairment Diagram

.center[.half[![Network Impairment](
assets/Impairment.png)]]

---
template: section_layout

Next Steps

---
# Next steps

* Deploy Ironic at edge to support bare metal instances for hosting K8S
* Use config-data for edge instances, VMs and BM
 * Allows for IP injection etc
* Deploy OpenShift on bare metal instances at edge site.
* Completion around Train/Shanghai time frame
---

template: thank_you

# Q&A

## Thank you!

**David Paterson**
![email](assets/email.png)
[david.paterson@dell.com](mailto:david.paterson@dell.com)
![irc](assets/irc.png)
[dpaterson (freenode)](irc://chat.freenode.net/dpaterson,isnick)


    </textarea>
    <script src="remark.js" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        slideNumberFormat: '%current%  <span class="designator">OpenStack at the Edge - Distributed Compute Nodes (DCN) · May  2019 · Red Hat Summit</span>',
        countIncrementalSlides: false
      });
    </script>
  </body>
</html>

<!-- vim: set ft=markdown : -->
